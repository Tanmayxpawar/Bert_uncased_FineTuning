# ğŸ›¡ï¸ Phishing URL Detection Using BERT

## ğŸ¯ About
This project implements a machine learning model for detecting phishing URLs using a fine-tuned BERT model. The system analyzes URLs to determine whether they are legitimate or potentially malicious phishing attempts. Built on the `bert-base-uncased` architecture from Google, this model achieves high accuracy in distinguishing between safe and unsafe URLs.

## â­ Key Features
- ğŸ¯ Binary classification of URLs (Safe/Not Safe)
- ğŸ¤– Fine-tuned BERT model with frozen base layers
- ğŸ“ˆ High performance metrics (ROC-AUC and Accuracy)
- ğŸ”Œ Easy-to-use interface for URL classification
- âš¡ Efficient preprocessing pipeline
- ğŸ’ª Built-in confidence scoring

## ğŸ”§ Technical Details

### ğŸ—ï¸ Model Architecture
- ğŸ”® Base Model: `google-bert/bert-base-uncased`
- ğŸ¯ Classification Head: Binary classification (Safe/Not Safe)
- ğŸ› ï¸ Optimization: Base model layers frozen except for pooling layers
- ğŸ“ Training Strategy: Fine-tuning approach with selective layer freezing

### ğŸ“Š Dataset
ğŸ—‚ï¸ The model is trained on the "shawhin/phishing-site-classification" dataset from Hugging Face, which contains labeled examples of both legitimate and phishing URLs.

### ğŸ“ˆ Training Metrics
#### ğŸ† Final Training Outputs
- ğŸ”„ Total Training Steps: 2,630
- ğŸ“‰ Final Training Loss: 0.3581
- â±ï¸ Training Runtime: 1018.05 seconds
- ğŸš€ Training Throughput: 20.63 samples/second
- âš¡ Training Steps/Second: 2.58
- ğŸ’» Total FLOPS: 7.07e14

#### ğŸ“Š Epoch-wise Performance
| Epoch | Training Loss | Validation Loss | Accuracy | AUC     |
|-------|---------------|-----------------|----------|---------|
| 1     | 0.4940       | 0.3772          | 0.8160   | 0.9130  |
| 2     | 0.4058       | 0.3677          | 0.8400   | 0.9340  |
| 3     | 0.3687       | 0.3245          | 0.8560   | 0.9350  |
| 4     | 0.3460       | 0.4032          | 0.8380   | 0.9440  |
| 5     | 0.3501       | 0.3123          | 0.8710   | 0.9450  |
| 6     | 0.3522       | 0.2904          | 0.8620   | 0.9500  |
| 7     | 0.3217       | 0.3069          | 0.8620   | 0.9470  |
| 8     | 0.3109       | 0.2940          | 0.8640   | 0.9490  |
| 9     | 0.3214       | 0.2852          | 0.8730   | 0.9500  |
| 10    | 0.3097       | 0.2974          | 0.8710   | 0.9510  |

### ğŸ¯ Performance Highlights
- ğŸ† Best Accuracy: 0.8730 (Epoch 9)
- ğŸŒŸ Best AUC Score: 0.9510 (Epoch 10)
- ğŸ“‰ Lowest Validation Loss: 0.2852 (Epoch 9)
- ğŸ¯ Final Model Metrics:
  - âœ… Accuracy: 0.8710
  - ğŸ“ˆ AUC: 0.9510
  - ğŸ“‰ Validation Loss: 0.2974

### âš™ï¸ Training Configuration
- ğŸ“Š Learning Rate: 2e-4
- ğŸ“¦ Batch Size: 8
- ğŸ”„ Training Epochs: 10
- ğŸ“‹ Evaluation Strategy: Per epoch
- âœ¨ Best Model Selection: Enabled

## ğŸš€ Usage

### ğŸ“¥ Installation Requirements
```bash
pip install transformers datasets evaluate numpy torch accelerate
```

### ğŸ’» Basic Usage
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# Load the model
model = AutoModelForSequenceClassification.from_pretrained("bert-phishing-classifier")
tokenizer = AutoTokenizer.from_pretrained("bert-phishing-classifier")

# Classify a URL
url = "http://example.com"
label, confidence = classify_url(url)
print(f"Classification: {label}, Confidence: {confidence:.2f}%")
```

### ğŸ”„ Model Training Process
1. ğŸ“¥ Dataset loading and preprocessing
2. ğŸ¯ Model initialization with frozen layers
3. âš™ï¸ Training configuration setup
4. ğŸƒ Model training with evaluation
5. ğŸ’¾ Model saving for future use

## ğŸ“ Implementation Notes
- ğŸ” Custom preprocessing function for URL tokenization
- ğŸ›‘ Early stopping and best model saving
- ğŸ“¦ Efficient batch processing with data collation
- ğŸ“Š Comprehensive evaluation metrics

## ğŸš€ Future Improvements
- ğŸŒ Multi-language support
- ğŸ” More sophisticated URL preprocessing techniques
- ğŸ¯ Regular expression-based features
- ğŸ“ˆ Training dataset expansion
- ğŸ¤ Ensemble methods implementation

## ğŸ“œ License
[Specify your license here]

## ğŸ‘¥ Contributors
[Add contributor information here]

## ğŸ“š Citation
If you use this model in your research, please cite:
```
[Add citation information here]
```